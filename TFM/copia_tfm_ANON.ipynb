{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFM: De Neo4j a Power BI: Integración de Datos para la trazabilidad y análisis de la industria médica\n",
    "\n",
    "El presente notebook contiene el código en Python desarrollado para la realización del\n",
    "Análisis Exploratorio de Datos (EDA) y el análisis técnico del Trabajo Fin de Máster (TFM).\n",
    "\n",
    "### Nota sobre confidencialidad y anonimización\n",
    "\n",
    "Debido a que los datos utilizados en este trabajo provienen de un entorno industrial real,\n",
    "no es posible compartir información real, confidencial ni propietaria de ninguna empresa.\n",
    "Por este motivo, los datos, nombres de variables, identificadores de productos, procesos\n",
    "y cualquier referencia sensible han sido debidamente anonimizados o agregados con fines\n",
    "exclusivamente académicos.\n",
    "\n",
    "El objetivo de este código es demostrar la metodología, la lógica de análisis,\n",
    "las técnicas empleadas y la estructura del flujo de datos, sin comprometer la\n",
    "confidencialidad de la información original.\n",
    "\n",
    "En consecuencia, los archivos de datos originales no se incluyen en este repositorio.\n",
    "El énfasis del trabajo se centra en el diseño del análisis, la ingeniería de datos\n",
    "y la interpretación de resultados, y no en la divulgación del conjunto de datos utilizado.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proceso ETL y modelado en Neo4j\n",
    "\n",
    "El proceso de Extracción, Transformación y Carga (ETL) constituye la base del presente\n",
    "trabajo, permitiendo integrar información proveniente de distintas fuentes operativas\n",
    "en un modelo de datos orientado a grafos.\n",
    "\n",
    "Dado el volumen y la complejidad del pipeline implementado, en este notebook se presenta\n",
    "una visión de alto nivel del proceso ETL, centrándose en su lógica general y en el modelo\n",
    "conceptual de datos, sin entrar en detalles de implementación específicos.\n",
    "\n",
    "El objetivo del ETL es transformar datos heterogéneos en una estructura de grafo en Neo4j,\n",
    "donde entidades como partes, procesos, defectos y datapoints quedan conectadas mediante\n",
    "relaciones que permiten realizar análisis exploratorios y consultas complejas de forma\n",
    "eficiente.\n",
    "\n",
    "Este enfoque facilita la trazabilidad, el análisis de relaciones causa-efecto y la\n",
    "exploración de patrones que no son evidentes en modelos tabulares tradicionales.\n",
    "\n",
    "\n",
    "# Orquestación ETL con Airflow (DAG)\n",
    "\n",
    "Para poblar el grafo, se implementó un DAG en Apache Airflow que ejecuta diariamente un flujo ETL. El DAG separa el proceso en tres etapas principales:\n",
    "(1) Extract: consulta fuentes relacionales ( Data Warehouses) mediante SQL y genera DataFrames en pandas.\n",
    "(2) Transform (Clean): estandariza tipos, fechas, nulos, duplicados y aplica reglas de negocio necesarias (por ejemplo, agrupar transacciones por lote y sumar cantidades).\n",
    "(3) Load: construye el modelo en Neo4j creando/actualizando nodos y relaciones mediante Cypher en batch (UNWIND), asegurando idempotencia con MERGE.\n",
    "\n",
    "En producción, credenciales y endpoints se administran con prácticas de IT (secrets/vault y variables seguras de Airflow). Para fines académicos, se muestran variables neutrales y se omiten identificadores reales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from neo4j import GraphDatabase\n",
    "\n",
    "NEO4J_URI = \"neo4j+s://<ANON_HOST>:7687\"\n",
    "NEO4J_USER = \"<ANON_USER>\"\n",
    "NEO4J_PASSWORD = \"<ANON_PASSWORD>\"\n",
    "\n",
    "# ============================================================\n",
    "# ETL (Extract – Clean – Transform/Load) hacia Neo4j\n",
    "# ============================================================\n",
    "\"\"\"\n",
    "Resumen ETL del pipeline:\n",
    "\n",
    "1) EXTRACT:\n",
    "   - Se consulta un origen relacional (p. ej., MES/ERP/DWH en SQL) y se carga en pandas.\n",
    "\n",
    "2) CLEAN:\n",
    "   - Se estandarizan fechas, tipos, nulos, duplicados y reglas básicas de negocio.\n",
    "\n",
    "3) TRANSFORM/LOAD:\n",
    "   - Se construyen nodos y relaciones en Neo4j usando Cypher con UNWIND para cargar en batch.\n",
    "\"\"\"\n",
    "\n",
    "def extract_sql_to_df(sql_query: str, sql_conn) -> pd.DataFrame:\n",
    "    \"\"\"EXTRACT: SQL -> pandas (conexión gestionada fuera).\"\"\"\n",
    "    return pd.read_sql(sql_query, con=sql_conn)\n",
    "\n",
    "def clean_df(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"CLEAN: limpieza genérica en pandas.\"\"\"\n",
    "    df = df.copy()\n",
    "    if \"event_time\" in df.columns:\n",
    "        df[\"event_time\"] = pd.to_datetime(df[\"event_time\"], errors=\"coerce\").dt.strftime(\"%Y-%m-%dT%H:%M:%S\")\n",
    "    if \"quantity\" in df.columns:\n",
    "        df[\"quantity\"] = pd.to_numeric(df[\"quantity\"], errors=\"coerce\").fillna(0)\n",
    "    return df.drop_duplicates()\n",
    "\n",
    "def load_df_to_neo4j(df: pd.DataFrame):\n",
    "    \"\"\"TRANSFORM/LOAD: pandas -> Neo4j con UNWIND.\"\"\"\n",
    "    cypher = \"\"\"\n",
    "    UNWIND $rows AS row\n",
    "\n",
    "    MERGE (p:Part {batch_id: toString(row.batch_id)})\n",
    "    ON CREATE SET p.material_id = toString(row.material_id)\n",
    "\n",
    "    MERGE (s:Scrap {container_batch: toString(row.container_batch), reason_code: toString(row.reason_code)})\n",
    "    ON CREATE SET s.event_time = datetime(row.event_time),\n",
    "                  s.quantity   = toInteger(row.quantity)\n",
    "\n",
    "    MERGE (p)-[:GENERATES]->(s)\n",
    "    \"\"\"\n",
    "\n",
    "    rows = df.to_dict(\"records\")\n",
    "    driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASSWORD))\n",
    "    with driver.session() as session:\n",
    "        session.run(cypher, rows=rows)\n",
    "    driver.close()\n",
    "\n",
    "# Ejemplo (comentado):\n",
    "# sql = \"SELECT batch_id, material_id, container_batch, reason_code, event_time, quantity FROM <ANON_TABLE> WHERE ...\"\n",
    "# df_raw = extract_sql_to_df(sql, sql_conn)\n",
    "# df_clean = clean_df(df_raw)\n",
    "# load_df_to_neo4j(df_clean)\n",
    "\n",
    "# ============================================================\n",
    "# Airflow DAG template (pipeline completo, anonimiz.)\n",
    "# ============================================================\n",
    "\"\"\"\n",
    "Este DAG ilustra la orquestación de un pipeline ETL hacia Neo4j.\n",
    "Se muestra como plantilla (template) y se omiten detalles sensibles.\n",
    "\n",
    "Estructura típica:\n",
    "- Task 1: Extract (SQL)\n",
    "- Task 2: Clean (pandas)\n",
    "- Task 3: Load (Neo4j)\n",
    "- Opcional: Score / métricas / validaciones de integridad\n",
    "\n",
    "Nota:\n",
    "- Variables/secretos se obtienen desde Airflow Variables/Connections o un Vault (no en el código).\n",
    "\"\"\"\n",
    "\n",
    "import pendulum\n",
    "from airflow.decorators import dag, task\n",
    "\n",
    "@dag(\n",
    "    schedule_interval=\"0 8 * * *\",\n",
    "    start_date=pendulum.datetime(2024, 4, 16, tz=\"America/Costa_Rica\"),\n",
    "    catchup=False,\n",
    "    tags=[\"Production\", \"Data_Science\", \"Neo4j\", \"ETL\"],\n",
    ")\n",
    "def neo4j_etl_pipeline():\n",
    "\n",
    "    @task\n",
    "    def extract_sql():\n",
    "        \"\"\"\n",
    "        EXTRACT:\n",
    "        - Ejecuta queries SQL contra fuentes tabulares (ej. MES/DWH)\n",
    "        - Devuelve dataframes/raw tables para transformar\n",
    "        \"\"\"\n",
    "        # df_ci = run_query(sql_component_issue)\n",
    "        # df_tc = run_query(sql_task_completions)\n",
    "        # df_scrap = run_query(sql_scrap)\n",
    "        return \"raw_extracted_data\"\n",
    "\n",
    "    @task\n",
    "    def transform_pandas(raw_data):\n",
    "        \"\"\"\n",
    "        TRANSFORM:\n",
    "        - Limpieza y estandarización (fechas, tipos, nulos)\n",
    "        - Normalización de llaves (ej. container_batch)\n",
    "        - Agregaciones necesarias para carga eficiente\n",
    "        \"\"\"\n",
    "        # df_clean = clean_and_group(raw_data)\n",
    "        return \"clean_transformed_data\"\n",
    "\n",
    "    @task\n",
    "    def load_neo4j(clean_data):\n",
    "        \"\"\"\n",
    "        LOAD:\n",
    "        - Crea/actualiza nodos y relaciones en Neo4j (Cypher + MERGE)\n",
    "        - Cargas por lotes (batching) para performance\n",
    "        \"\"\"\n",
    "        # write_nodes_and_rels(clean_data)\n",
    "        return \"done\"\n",
    "\n",
    "    raw = extract_sql()\n",
    "    clean = transform_pandas(raw)\n",
    "    load_neo4j(clean)\n",
    "\n",
    "dag = neo4j_etl_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracción de datos de neo4j\n",
    "\n",
    "En este TFM se utiliza **Neo4j**, una base de datos orientada a grafos (graph database).  \n",
    "A diferencia de una base relacional (tablas), en Neo4j la información se representa mediante:\n",
    "\n",
    "- **Nodos**: entidades (por ejemplo, `part`, `process`, `scrap`, `datapoint`)\n",
    "- **Relaciones**: conexiones entre entidades (por ejemplo, una parte *genera* scrap, o un datapoint *pertenece* a una parte)\n",
    "\n",
    "Para consultar y recorrer el grafo se emplea **Cypher**, el lenguaje de consultas de Neo4j.  \n",
    "Cypher permite:\n",
    "- filtrar nodos por propiedades (ej. `scrap_reason_code = 'HP12'`)\n",
    "- recorrer relaciones para unir información de diferentes entidades (traversals)\n",
    "- seleccionar variables relevantes para el análisis (EDA y validación de hipótesis)\n",
    "\n",
    "A continuación se documentan las consultas Cypher utilizadas para extraer subconjuntos de datos desde Neo4j, los cuales alimentan el análisis exploratorio y las visualizaciones del estudio.\n",
    "\n",
    "Neo4j ofrece una version de escritorio donde se puede ejecutar las consultas o \"queries\" directamente en el gráfico para este notebook, las consultas se hicieron através de python por lo tanto se importa la librería de neo4j y se crea una variable de gráfico, como es anónimo las credenciales aquí colocadas en la práctica son usuarios y contraseñas, y la dirección IP del cloud donde el gráfico esta en producción. Por motivos de confidencialidad, los identificadores de conexión y las credenciales han sido sustituidos por nombres de variables neutrales, sin exponer información real del entorno productivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j import GraphDatabase\n",
    "\n",
    "\n",
    "NEO4J_URI = \"neo4j+s://<ANON_HOST>:7687\"\n",
    "NEO4J_USER = \"<ANON_USER>\"\n",
    "NEO4J_PASSWORD = \"<ANON_PASSWORD>\"\n",
    "\n",
    "graph = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASSWORD))\n",
    "\n",
    "# --- Query 1: Scrap por código de defecto (tendencia temporal)\n",
    "query1 = \"\"\"\n",
    "MATCH (s:scrap)\n",
    "WHERE s.scrap_reason_code = $scrap_code\n",
    "RETURN s.quantity, s.scrap_date, s.container_batch\n",
    "ORDER BY s.scrap_date ASC\n",
    "\"\"\"\n",
    "\n",
    "# --- Query 2: Bond test + part + scrap para lotes específicos\n",
    "query2 = \"\"\"\n",
    "MATCH (dp:dp_bondtest)-[]->(p:part)-[]->(s:scrap)\n",
    "MATCH (pr:process)-[]->(p:part)\n",
    "WHERE dp.datapoint_name = $datapoint_name\n",
    "  AND s.scrap_reason_code = $scrap_code\n",
    "  AND s.container_batch IN $batches\n",
    "RETURN dp.data_value, dp.submitter_name, s.scrap_date, s.quantity, s.container_batch,\n",
    "       pr.workstation, pr.tasklist_name\n",
    "ORDER BY s.scrap_date\n",
    "\"\"\"\n",
    "\n",
    "# --- Query 3: Defectos asociados a materiales específicos\n",
    "query3 = \"\"\"\n",
    "MATCH (p:part)-[:GENERATES]->(s:scrap)\n",
    "WHERE s.scrap_reason_code = $scrap_code\n",
    "  AND p.material_id IN $materials\n",
    "RETURN p.batch_id AS batch,\n",
    "       p.material_id AS material,\n",
    "       s.scrap_date AS scrap_date,\n",
    "       s.quantity AS scrap_qty\n",
    "\"\"\"\n",
    "\n",
    "params_common = {\"scrap_code\": \"HP12\"}\n",
    "\n",
    "with graph.session() as session:\n",
    "    df1 = session.run(query1, **params_common).to_data_frame()\n",
    "\n",
    "    df2 = session.run(\n",
    "        query2,\n",
    "        **params_common,\n",
    "        datapoint_name=\"Valor bond test ?*\",\n",
    "        batches=[\"loteA\", \"loteB\", \"loteC\"]\n",
    "    ).to_data_frame()\n",
    "\n",
    "    df3 = session.run(\n",
    "        query3,\n",
    "        **params_common,\n",
    "        materials=[\"mat_a_01\", \"mat_a_02\", \"mat_b_01\", \"mat_b_02\",\n",
    "                   \"mat_c_01\", \"mat_c_02\", \"mat_d_01\", \"mat_d_02\"]\n",
    "    ).to_data_frame()\n",
    "\n",
    "graph.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leer el conjunto de datos exportado desde Neo4j\n",
    "tfm1 = pd.read_csv(r\"./data/export_tfm.csv\")\n",
    "tfm1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agrupamiento de los datos\n",
    "\n",
    "tfm1['s.scrap_date'] = pd.to_datetime(tfm1['s.scrap_date'])\n",
    "tfm1 = tfm1.sort_values('s.scrap_date')\n",
    "daily = tfm1.groupby(tfm1['s.scrap_date'].dt.date)['s.quantity'].sum().reset_index()\n",
    "tfm1['s.quantity'] = tfm1['s.quantity'] * -1\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(daily['s.scrap_date'], daily['s.quantity'], marker='o')\n",
    "plt.title('Tendencia del defecto DEFECT_1 a lo largo del tiempo')\n",
    "plt.xlabel('Fecha')\n",
    "plt.ylabel('Cantidad de srap diario')\n",
    "plt.grid(True)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lectura del conjunto de datos exportado desde Neo4j consulta 2\n",
    "tfm2 = pd.read_csv(r\"./data/export_tfm2.csv\")\n",
    "tfm2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asegurar el orden de las fechas y se convertir a formato numérico para análisis adicional\n",
    "tfm2['s.scrap_date'] = pd.to_datetime(tfm2['s.scrap_date'])\n",
    "tfm2 = tfm2.sort_values('s.scrap_date')\n",
    "tfm2['time_numeric'] = tfm2['s.scrap_date'].apply(lambda x: x.toordinal())\n",
    "tfm2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gráfica de la tendencia del bond test a lo largo del tiempo\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(tfm2['s.scrap_date'], tfm2['dp.data_value'], marker='.', alpha=0.6)\n",
    "plt.title(\"Tendencia de la fuerza de adhesión lo largo del tiempo\")\n",
    "plt.xlabel(\"Fecha\")\n",
    "plt.ylabel(\"Valor de Bond Test\")\n",
    "plt.grid(True)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ===============================\n",
    "# 1. Cargar CSV\n",
    "# ===============================\n",
    "tfm2 = pd.read_csv(r\"./data/export_tfm2.csv\")\n",
    "\n",
    "# ===============================\n",
    "# 2. Convertir fecha\n",
    "# ===============================\n",
    "tfm2['s.scrap_date'] = pd.to_datetime(tfm2['s.scrap_date'], errors='coerce')\n",
    "\n",
    "# ===============================\n",
    "# 3. LIMPIEZA COMPLETA DE dp.data_value\n",
    "# ===============================\n",
    "\n",
    "# Convertir todo a string\n",
    "tfm2['dp.data_value'] = tfm2['dp.data_value'].astype(str)\n",
    "\n",
    "# Mantener solo números y puntos\n",
    "tfm2['dp.data_value'] = tfm2['dp.data_value'].str.replace(r'[^0-9.]', '', regex=True)\n",
    "\n",
    "# Colapsar múltiples puntos \"..\" o \"....\" → \".\"\n",
    "tfm2['dp.data_value'] = tfm2['dp.data_value'].str.replace(r'\\.+', '.', regex=True)\n",
    "\n",
    "# Eliminar puntos al inicio o final\n",
    "tfm2['dp.data_value'] = tfm2['dp.data_value'].str.strip('.')\n",
    "\n",
    "# Convertir a número real\n",
    "tfm2['dp.data_value'] = pd.to_numeric(tfm2['dp.data_value'], errors='coerce')\n",
    "\n",
    "# ===============================\n",
    "# 4. Eliminar outliers imposibles\n",
    "# ===============================\n",
    "# Valores reales del Bond Test están entre ~4 y 6\n",
    "tfm2 = tfm2[(tfm2['dp.data_value'] >= 0) & (tfm2['dp.data_value'] <= 20)]\n",
    "\n",
    "# ===============================\n",
    "# 5. Ordenar por fecha\n",
    "# ===============================\n",
    "tfm2 = tfm2.sort_values('s.scrap_date')\n",
    "\n",
    "# ===============================\n",
    "# 6. Agrupar por día y sacar promedio\n",
    "# ===============================\n",
    "daily_bond = (\n",
    "    tfm2\n",
    "    .groupby(tfm2['s.scrap_date'].dt.date)['dp.data_value']\n",
    "    .mean()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# ===============================\n",
    "# 7. Graficar tendencia diaria\n",
    "# ===============================\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(daily_bond['s.scrap_date'], daily_bond['dp.data_value'], marker='o', alpha=0.7)\n",
    "plt.title(\"Tendencia diaria del Fuerza de Adhesión a lo largo del tiempo\")\n",
    "plt.xlabel(\"Fecha\")\n",
    "plt.ylabel(\"Promedio diario de la fuerza de Adhesión\")\n",
    "plt.grid(True)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# ==============================================\n",
    "# 1. Cargar CSV\n",
    "# ==============================================\n",
    "tfm2 = pd.read_csv(r\"./data/export_tfm2.csv\")\n",
    "\n",
    "# ==============================================\n",
    "# 2. Convertir fecha\n",
    "# ==============================================\n",
    "tfm2['s.scrap_date'] = pd.to_datetime(tfm2['s.scrap_date'], errors='coerce')\n",
    "\n",
    "# ==============================================\n",
    "# 3. LIMPIEZA COMPLETA DE dp.data_value\n",
    "# ==============================================\n",
    "\n",
    "# Convertir a string\n",
    "tfm2['dp.data_value'] = tfm2['dp.data_value'].astype(str)\n",
    "\n",
    "# Mantener solo números y puntos\n",
    "tfm2['dp.data_value'] = tfm2['dp.data_value'].str.replace(r'[^0-9.]', '', regex=True)\n",
    "\n",
    "# Colapsar múltiples puntos\n",
    "tfm2['dp.data_value'] = tfm2['dp.data_value'].str.replace(r'\\.+', '.', regex=True)\n",
    "\n",
    "# Quitar puntos al inicio o final\n",
    "tfm2['dp.data_value'] = tfm2['dp.data_value'].str.strip('.')\n",
    "\n",
    "# Convertir a número\n",
    "tfm2['dp.data_value'] = pd.to_numeric(tfm2['dp.data_value'], errors='coerce')\n",
    "\n",
    "# ==============================================\n",
    "# 4. Eliminar outliers físicamente imposibles\n",
    "# ==============================================\n",
    "tfm2 = tfm2[(tfm2['dp.data_value'] >= 0) & (tfm2['dp.data_value'] <= 20)]\n",
    "\n",
    "# ==============================================\n",
    "# 5. ANONIMIZAR OPERADORES\n",
    "# ==============================================\n",
    "\n",
    "unique_ops = tfm2['dp.submitter_name'].unique()\n",
    "anon_map = {op: f\"Operador_{i+1}\" for i, op in enumerate(unique_ops)}\n",
    "\n",
    "tfm2['submitter_anon'] = tfm2['dp.submitter_name'].map(anon_map)\n",
    "\n",
    "# ==============================================\n",
    "# 6. GRAFICO: BOX PLOT POR OPERADOR ANÓNIMO\n",
    "# ==============================================\n",
    "\n",
    "plt.figure(figsize=(16,8))\n",
    "sns.boxplot(\n",
    "    data=tfm2,\n",
    "    x='submitter_anon',\n",
    "    y='dp.data_value'\n",
    ")\n",
    "\n",
    "plt.title(\"Distribución de la Fuerza de Adhesión por Operador (Anonimizado)\\nCasos asociados a Scrap DEFECT_1\")\n",
    "plt.xlabel(\"Operador\")\n",
    "plt.ylabel(\"Valor de la fuerza de adhesión\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.grid(axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Reemplazar en el dataframe\n",
    "tfm2['submitter_anon'] = tfm2['dp.submitter_name'].map(anon_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import f_oneway\n",
    "\n",
    "# Crear listas de valores por operador\n",
    "groups = [group[\"dp.data_value\"].values \n",
    "          for name, group in tfm2.groupby(\"submitter_anon\") \n",
    "          if len(group) > 1]  # solo operadores con más de 1 dato para evitar errores\n",
    "\n",
    "# ANOVA de una vía\n",
    "anova_result = f_oneway(*groups)\n",
    "\n",
    "anova_result\n",
    "\n",
    "\n",
    "print(\"ANOVA F-statistic:\", anova_result.statistic)\n",
    "print(\"ANOVA p-value:\", anova_result.pvalue)\n",
    "\n",
    "\n",
    "plt.hist(tfm2['dp.data_value'], bins=30)\n",
    "plt.title(\"Histograma de Valores de Fuerza de Adhesión\")\n",
    "plt.xlabel(\"Valor de Fuerza de Adhesión\")\n",
    "plt.ylabel(\"Frecuencia\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Tukey HSD: requiere que NO haya NaN\n",
    "df_clean = tfm2[['dp.data_value', 'submitter_anon']].dropna()\n",
    "\n",
    "# Ejecutar Tukey\n",
    "tukey_result = pairwise_tukeyhsd(\n",
    "    endog=df_clean['dp.data_value'],   # valores de Bond Test\n",
    "    groups=df_clean['submitter_anon'], # los operadores anonimizados\n",
    "    alpha=0.05\n",
    ")\n",
    "\n",
    "print(tukey_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = tukey_result.plot_simultaneous(figsize=(14, 10))\n",
    "\n",
    "ax = plt.gca()\n",
    "\n",
    "# Cambiar color de líneas y puntos\n",
    "for line in ax.lines:\n",
    "    line.set_color(\"#4DA3D9\")\n",
    "\n",
    "# Cambiar color de textos (opcional, más prolijo)\n",
    "ax.title.set_color(\"#374151\")\n",
    "ax.xaxis.label.set_color(\"#374151\")\n",
    "ax.yaxis.label.set_color(\"#374151\")\n",
    "\n",
    "plt.title(\"Comparaciones Post-Hoc Tukey HSD entre Operadores\")\n",
    "plt.xlabel(\"Media de la fuerza de adhesión\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# relacion entre la fuerza de adhesion  y el scrap en cantidad absoluta\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Crear columna de scrap absoluto\n",
    "tfm2[\"scrap_abs\"] = tfm2[\"s.quantity\"].abs()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.scatter(\n",
    "    tfm2[\"dp.data_value\"],\n",
    "    tfm2[\"scrap_abs\"],\n",
    "    color=\"#4DA3D9\",\n",
    "    alpha=0.7\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Data Value (dp.data_value)\", color=\"#374151\")\n",
    "plt.ylabel(\"Cantidad de Scrap (valor absoluto)\", color=\"#374151\")\n",
    "plt.title(\"Relación entre Data Value y Cantidad de Scrap\", color=\"#374151\")\n",
    "\n",
    "plt.grid(color=\"#E5E7EB\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ================================================\n",
    "# 1. Limpieza básica: scrap absoluto\n",
    "# ================================================\n",
    "tfm2['scrap_abs'] = tfm2['s.quantity'].abs()\n",
    "\n",
    "# Asegurar que la fecha es datetime\n",
    "tfm2['s.scrap_date'] = pd.to_datetime(tfm2['s.scrap_date'], errors='coerce')\n",
    "\n",
    "# ================================================\n",
    "# 2. Consolidar SCRAP REAL por batch (max)\n",
    "#    y la fecha asociada al scrap (min o max)\n",
    "# ================================================\n",
    "scrap_fecha = (\n",
    "    tfm2.groupby('s.container_batch')\n",
    "        .agg({\n",
    "            'scrap_abs': 'max',       # cantidad REAL de scrap por batch\n",
    "            's.scrap_date': 'min'     # primera fecha del scrap en ese batch\n",
    "        })\n",
    "        .sort_values('scrap_abs', ascending=False)\n",
    ")\n",
    "\n",
    "print(\"Scrap consolidado por batch:\")\n",
    "print(scrap_fecha.head(20))\n",
    "\n",
    "# ================================================\n",
    "# 3. Top 20 batches\n",
    "# ================================================\n",
    "top20 = scrap_fecha.head(20)\n",
    "\n",
    "# ================================================\n",
    "# 4. Gráfico de barras: Top 20\n",
    "# ================================================\n",
    "plt.figure(figsize=(14,7))\n",
    "sns.barplot(\n",
    "    x=top20.index.astype(str),\n",
    "    y=top20['scrap_abs'],\n",
    "    palette='Blues_r'\n",
    ")\n",
    "plt.title(\"Top 20 Batches con Mayor Scrap DEFECT_1 (Valor Único por Batch)\", fontsize=16)\n",
    "plt.xlabel(\"Batch\", fontsize=14)\n",
    "plt.ylabel(\"Cantidad de Scrap DEFECT_1\", fontsize=14)\n",
    "plt.xticks(rotation=90)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.6)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ================================================\n",
    "# 5. Scatter Fecha vs Scrap (bivariado)\n",
    "# ================================================\n",
    "plt.figure(figsize=(14,7))\n",
    "plt.scatter(scrap_fecha['s.scrap_date'], scrap_fecha['scrap_abs'], alpha=0.7)\n",
    "plt.title(\"Relación entre Fecha y Scrap DEFECT_1 por Batch\", fontsize=16)\n",
    "plt.xlabel(\"Fecha del Scrap\", fontsize=14)\n",
    "plt.ylabel(\"Scrap DEFECT_1 (valor único por batch)\", fontsize=14)\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ================================================\n",
    "# 6. Timeline estilo \"stem plot\"\n",
    "# ================================================\n",
    "plt.figure(figsize=(14,6))\n",
    "plt.stem(scrap_fecha['s.scrap_date'], scrap_fecha['scrap_abs'], basefmt=\" \")\n",
    "plt.title(\"Timeline de Scrap DEFECT_1 por Batch\", fontsize=16)\n",
    "plt.xlabel(\"Fecha\", fontsize=14)\n",
    "plt.ylabel(\"Scrap DEFECT_1\", fontsize=14)\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 1) Preparar datos\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "# Asegurar que scrap_date es datetime\n",
    "tfm2['s.scrap_date'] = pd.to_datetime(tfm2['s.scrap_date'])\n",
    "\n",
    "# Agrupar por batch: scrap total (abs sum) + fecha mínima del batch\n",
    "batch_group = (\n",
    "    tfm2.groupby('s.container_batch')\n",
    "        .agg({\n",
    "            's.quantity': lambda x: x.abs().sum(),\n",
    "            's.scrap_date': 'min'\n",
    "        })\n",
    "        .reset_index()\n",
    ")\n",
    "\n",
    "# Crear columna mes-año\n",
    "batch_group['mes_anio'] = batch_group['s.scrap_date'].dt.to_period('M').astype(str)\n",
    "\n",
    "# Seleccionar Top 20 batches\n",
    "top20 = batch_group.sort_values(by='s.quantity', ascending=False).head(20)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 2) GRAFICAR\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "plt.figure(figsize=(18, 7))\n",
    "\n",
    "# Barras\n",
    "bars = plt.bar(\n",
    "    top20['s.container_batch'].astype(str),\n",
    "    top20['s.quantity'],\n",
    "    color=plt.cm.Blues_r(range(20))\n",
    ")\n",
    "\n",
    "plt.title(\"Top 20 Lotes con Mayor Scrap DEFECT_1 (con mes y año del lote)\", fontsize=18)\n",
    "plt.xlabel(\"Lote\", fontsize=14)\n",
    "plt.ylabel(\"Cantidad de Scrap DEFECT_1\", fontsize=14)\n",
    "plt.xticks(rotation=90, fontsize=10)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 3) Insertar fecha dentro de cada barra (centrado)\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "for bar, fecha in zip(bars, top20['mes_anio']):\n",
    "    height = bar.get_height()\n",
    "    plt.text(\n",
    "        bar.get_x() + bar.get_width() / 2,   # centro horizontal\n",
    "        height / 2,                          # centro vertical\n",
    "        fecha,\n",
    "        ha='center',\n",
    "        va='center',\n",
    "        rotation=90,\n",
    "        fontsize=10,\n",
    "        fontweight='bold',\n",
    "        color='white'                        # Contraste\n",
    "    )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Copia del dataframe original para no dañarlo\n",
    "df_turno = tfm2.copy()\n",
    "\n",
    "# --- 1. Convertir scrap_date a datetime\n",
    "df_turno['s.scrap_date'] = pd.to_datetime(df_turno['s.scrap_date'])\n",
    "\n",
    "# --- 2. Corregir scrap a positivo\n",
    "df_turno['scrap_qty'] = df_turno['s.quantity'].abs()\n",
    "\n",
    "# --- 3. Definir turnos\n",
    "def asignar_turno(hora):\n",
    "    if hora >= 6 and hora < 15.5:     # 6:00 — 15:29\n",
    "        return 'A'\n",
    "    elif hora >= 15.5 and hora < 22:  # 15:30 — 21:59\n",
    "        return 'B'\n",
    "    else:                             # 22:00 — 5:59\n",
    "        return 'C'\n",
    "\n",
    "df_turno['turno'] = df_turno['s.scrap_date'].dt.hour.apply(asignar_turno)\n",
    "\n",
    "# --- 4. Quedarnos solo con el valor máximo de scrap por batch (evita duplicados)\n",
    "df_batch_max = (\n",
    "    df_turno.groupby('s.container_batch')['scrap_qty']\n",
    "            .max()\n",
    "            .reset_index()\n",
    ")\n",
    "\n",
    "# Ahora necesitamos unir esto de nuevo para recuperar el turno:\n",
    "df_merged = df_batch_max.merge(\n",
    "    df_turno[['s.container_batch', 'turno']].drop_duplicates(),\n",
    "    on='s.container_batch',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# --- 5. Sumar el scrap total por turno\n",
    "scrap_por_turno = df_merged.groupby('turno')['scrap_qty'].sum().reset_index()\n",
    "\n",
    "# --- 6. Graficar\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.bar(scrap_por_turno['turno'], scrap_por_turno['scrap_qty'], color=['#4DA3D9', '#1F6FA8', '#A9D6F5'])\n",
    "\n",
    "plt.title('Scrap DEFECT_1 por Turno (con valores positivos y agrupado por Lote)')\n",
    "plt.xlabel('Turno')\n",
    "plt.ylabel('Cantidad total de Scrap DEFECT_1')\n",
    "\n",
    "# Mostrar valores arriba de cada barra\n",
    "for i, v in enumerate(scrap_por_turno['scrap_qty']):\n",
    "    plt.text(i, v + 50, str(int(v)), ha='center', fontweight='bold')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Copia del dataframe original\n",
    "df_box = tfm2.copy()\n",
    "\n",
    "# --- 1. Convertir scrap_date a datetime\n",
    "df_box['s.scrap_date'] = pd.to_datetime(df_box['s.scrap_date'])\n",
    "\n",
    "# --- 2. Asegurar que Bond Test sea numérico\n",
    "df_box['dp.data_value'] = pd.to_numeric(df_box['dp.data_value'], errors='coerce')\n",
    "\n",
    "# --- 3. Definir turnos por hora\n",
    "def asignar_turno(hora):\n",
    "    if 6 <= hora < 15.5:\n",
    "        return 'A'\n",
    "    elif 15.5 <= hora < 22:\n",
    "        return 'B'\n",
    "    else:\n",
    "        return 'C'\n",
    "\n",
    "df_box['turno'] = df_box['s.scrap_date'].dt.hour.apply(asignar_turno)\n",
    "\n",
    "# --- 4. Crear figura con 3 boxplots (uno por turno)\n",
    "plt.figure(figsize=(14,6))\n",
    "\n",
    "turnos = ['A', 'B', 'C']\n",
    "\n",
    "for i, t in enumerate(turnos):\n",
    "    plt.subplot(1, 3, i+1)\n",
    "    sns.boxplot(\n",
    "        data=df_box[df_box['turno'] == t],\n",
    "        y='dp.data_value',\n",
    "        color=['#4DA3D9', '#1F6FA8', '#A9D6F5'][i]\n",
    "    )\n",
    "    plt.title(f'Turno {t}')\n",
    "    plt.ylabel('Prueba de adhesión (lbf)')\n",
    "    plt.ylim(df_box['dp.data_value'].min() - 0.1,\n",
    "             df_box['dp.data_value'].max() + 0.1)\n",
    "\n",
    "plt.suptitle(\"Distribución de la prueba de Adhesión por Turno (Scrap DEFECT_1)\", fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import f_oneway\n",
    "\n",
    "# Extraer valores por turno\n",
    "bond_A = df_box[df_box['turno'] == 'A']['dp.data_value'].dropna()\n",
    "bond_B = df_box[df_box['turno'] == 'B']['dp.data_value'].dropna()\n",
    "bond_C = df_box[df_box['turno'] == 'C']['dp.data_value'].dropna()\n",
    "\n",
    "# ANOVA de un factor\n",
    "F_stat, p_value = f_oneway(bond_A, bond_B, bond_C)\n",
    "\n",
    "print(\"ANOVA por Turno\")\n",
    "print(\"F-statistic:\", F_stat)\n",
    "print(\"p-value:\", p_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "import pandas as pd\n",
    "\n",
    "# Crear dataframe para Tukey\n",
    "df_anova_turnos = df_box[['dp.data_value', 'turno']].dropna()\n",
    "\n",
    "# Tukey HSD\n",
    "tukey_turnos = pairwise_tukeyhsd(\n",
    "    endog=df_anova_turnos['dp.data_value'],\n",
    "    groups=df_anova_turnos['turno'],\n",
    "    alpha=0.05\n",
    ")\n",
    "\n",
    "print(tukey_turnos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "\n",
    "# Preparamos datos\n",
    "df_anova_turnos = df_box[['dp.data_value', 'turno']].dropna()\n",
    "\n",
    "# Tukey HSD\n",
    "tukey_turnos = pairwise_tukeyhsd(\n",
    "    endog=df_anova_turnos['dp.data_value'],\n",
    "    groups=df_anova_turnos['turno'],\n",
    "    alpha=0.05\n",
    ")\n",
    "\n",
    "# Convertir resultados a dataframe\n",
    "tukey_df = pd.DataFrame(data=tukey_turnos._results_table.data[1:], \n",
    "                        columns=tukey_turnos._results_table.data[0])\n",
    "\n",
    "# Crear columnas numéricas\n",
    "tukey_df['diff'] = tukey_df['meandiff'].astype(float)\n",
    "tukey_df['lower'] = tukey_df['lower'].astype(float)\n",
    "tukey_df['upper'] = tukey_df['upper'].astype(float)\n",
    "\n",
    "# --- Gráfico ---\n",
    "plt.figure(figsize=(10,6))\n",
    "\n",
    "# Dibujar intervalos\n",
    "for i in range(len(tukey_df)):\n",
    "    plt.plot([tukey_df['lower'][i], tukey_df['upper'][i]], [i, i], 'k-', lw=2)\n",
    "    plt.plot(tukey_df['diff'][i], i, 'o', color='blue')\n",
    "\n",
    "# Línea vertical en 0\n",
    "plt.axvline(0, color='red', linestyle='--')\n",
    "\n",
    "plt.yticks(range(len(tukey_df)), tukey_df['group1'] + \" vs \" + tukey_df['group2'])\n",
    "plt.xlabel(\"Diferencia de medias (Fuerza de Adhesión)\")\n",
    "plt.title(\"Tukey HSD – Comparación entre Turnos\")\n",
    "plt.grid(axis='x', linestyle='--', alpha=0.5)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analisis bi variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfm2['scrap_abs'] = tfm2['s.quantity'].abs()\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.scatterplot(data=tfm2, x=\"dp.data_value\", y=\"scrap_abs\", alpha=0.5)\n",
    "\n",
    "plt.title(\"Relación entre Fuerza de Adhesión y Cantidad de Scrap DEFECT_1\")\n",
    "plt.xlabel(\"Fuerza de Adhesión\")\n",
    "plt.ylabel(\"Scrap DEFECT_1 (cantidad absoluta)\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "tfm2[['dp.data_value', 'scrap_abs']].corr()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# --- Cargar archivo ---\n",
    "df_tfm3 = pd.read_csv(r\"./data/export_tfm2.csv\")\n",
    "\n",
    "# --- Limpieza de datos ---\n",
    "# Convertir bond test a float\n",
    "df_tfm3['dp.data_value'] = pd.to_numeric(df_tfm3['dp.data_value'], errors='coerce')\n",
    "\n",
    "# Scrap absoluto\n",
    "df_tfm3['s.quantity_abs'] = df_tfm3['s.quantity'].abs()\n",
    "\n",
    "# Eliminar filas con valores faltantes (muy importante)\n",
    "df_clean = df_tfm3.dropna(subset=['dp.data_value', 's.quantity_abs'])\n",
    "\n",
    "# --- 1. Correlación de Pearson ---\n",
    "pearson_corr, pearson_p = pearsonr(df_clean['dp.data_value'], df_clean['s.quantity_abs'])\n",
    "\n",
    "# --- 2. R² usando regresión lineal ---\n",
    "X = df_clean[['dp.data_value']]\n",
    "y = df_clean['s.quantity_abs']\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "r2 = model.score(X, y)\n",
    "\n",
    "# --- Imprimir resultados ---\n",
    "print(\"Correlación de Pearson:\", round(pearson_corr, 4))\n",
    "print(\"P-value de Pearson:\", pearson_p)\n",
    "print(\"R² del modelo lineal:\", round(r2, 4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asegurar dt\n",
    "df_tfm3['s.scrap_date'] = pd.to_datetime(df_tfm3['s.scrap_date'])\n",
    "df_tfm3['year_month'] = df_tfm3['s.scrap_date'].dt.to_period('M').astype(str)\n",
    "\n",
    "# Scrap absoluto\n",
    "df_tfm3['s.quantity_abs'] = df_tfm3['s.quantity'].abs()\n",
    "\n",
    "# Scrap total por mes\n",
    "scrap_mensual = df_tfm3.groupby('year_month')['s.quantity_abs'].sum()\n",
    "\n",
    "# Mes con más scrap\n",
    "mes_max = scrap_mensual.idxmax()\n",
    "mes_max\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mes_critico = df_tfm3[df_tfm3['year_month'] == mes_max].copy()\n",
    "df_mes_critico.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- 1. Preparar dataframe ---\n",
    "df_tfm3['s.scrap_date'] = pd.to_datetime(df_tfm3['s.scrap_date'])\n",
    "df_tfm3['s.quantity_abs'] = df_tfm3['s.quantity'].abs()\n",
    "\n",
    "# --- 2. Extraer mes crítico ---\n",
    "df_sep = df_tfm3[df_tfm3['s.scrap_date'].dt.to_period('M') == '2024-09'].copy()\n",
    "\n",
    "# --- 3. Crear columnas de día y hora ---\n",
    "df_sep['Fecha'] = df_sep['s.scrap_date'].dt.date\n",
    "df_sep['Hora'] = df_sep['s.scrap_date'].dt.hour\n",
    "\n",
    "# --- 4. Agrupar correctamente:\n",
    "#     Para cada día-hora y batch, obtener el scrap máximo\n",
    "df_grouped = (\n",
    "    df_sep.groupby(['Fecha', 'Hora', 's.container_batch'])['s.quantity_abs']\n",
    "    .max()   # ← EVITA la multiplicación x5\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# --- 5. Ahora agrupar por Fecha y Hora sumando LOTES únicos ---\n",
    "df_heat = (\n",
    "    df_grouped.groupby(['Fecha', 'Hora'])['s.quantity_abs']\n",
    "    .sum()   # suma real por hora sin duplicados\n",
    "    .unstack(fill_value=0)\n",
    ")\n",
    "\n",
    "# --- 6. Plot del heatmap ---\n",
    "\n",
    "\n",
    "plt.figure(figsize=(16, 9))\n",
    "\n",
    "sns.heatmap(\n",
    "    df_heat,\n",
    "    cmap='Blues',\n",
    "    linewidths=0.3,\n",
    "    linecolor='#E5E7EB',\n",
    "    cbar_kws={'label': 'Cantidad de Scrap (valor absoluto)'}\n",
    ")\n",
    "\n",
    "plt.title(\n",
    "    \"Heatmap de Scrap DEFECT_1 — Mes Crítico 2024-09\",\n",
    "    fontsize=16,\n",
    "    color=\"#374151\"\n",
    ")\n",
    "plt.xlabel(\"Hora del día\", color=\"#374151\")\n",
    "plt.ylabel(\"Fecha\", color=\"#374151\")\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tfm4 = pd.read_csv(r\"./data/export_tfm2.csv\")\n",
    "df_tfm4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# --- 1. Vista general ---\n",
    "print(\"Shape (filas, columnas):\")\n",
    "print(df_tfm4.shape)\n",
    "print(\"\\nPrimeras filas:\")\n",
    "display(df_tfm4.head())\n",
    "\n",
    "# --- 2. Tipos de datos ---\n",
    "print(\"\\nTipos de datos:\")\n",
    "print(df_tfm4.dtypes)\n",
    "\n",
    "# --- 3. Descripción estadística de las variables numéricas ---\n",
    "print(\"\\nDescripción estadística (numéricas):\")\n",
    "display(df_tfm4.describe())\n",
    "\n",
    "# --- 4. Cantidad total de data_value, min, max, mean ---\n",
    "print(\"\\nResumen de dp.data_value:\")\n",
    "print(\"Count  :\", df_tfm4['dp.data_value'].count())\n",
    "print(\"Min    :\", df_tfm4['dp.data_value'].min())\n",
    "print(\"Max    :\", df_tfm4['dp.data_value'].max())\n",
    "print(\"Mean   :\", df_tfm4['dp.data_value'].mean())\n",
    "\n",
    "# --- 5. ¿Cuántos operadores/submitters diferentes hay? ---\n",
    "print(\"\\nCantidad de submitters únicos:\")\n",
    "print(df_tfm4['dp.submitter_name'].nunique())\n",
    "\n",
    "print(\"\\nLista de submitters únicos:\")\n",
    "print(df_tfm4['dp.submitter_name'].unique())\n",
    "\n",
    "# --- 6. ¿Cuántos batches diferentes hay? ---\n",
    "print(\"\\nCantidad de container_batch únicos:\")\n",
    "print(df_tfm4['s.container_batch'].nunique())\n",
    "\n",
    "# --- 7. Valores únicos de scrap quantities (por curiosidad EDA) ---\n",
    "print(\"\\nValores distintos de s.quantity:\")\n",
    "print(df_tfm4['s.quantity'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df_mat_tfm = pd.read_csv(r\"./data/export_tfm3.csv\")\n",
    "df_mat_tfm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (df_mat_tfm['material'].unique())\n",
    "\n",
    "\n",
    "material_map = {\n",
    "    '808886-01': 'mat_a_01',\n",
    "    '808886-02': 'mat_a_02',\n",
    "\n",
    "    '809273-01': 'mat_b_01',\n",
    "    '809273-02': 'mat_b_02',\n",
    "\n",
    "    '809297-01': 'mat_c_01',\n",
    "    '809297-02': 'mat_c_02',\n",
    "\n",
    "    '809299-01': 'mat_d_01',\n",
    "    '809299-02': 'mat_d_02'\n",
    "}\n",
    "\n",
    "df_mat_tfm['material_anon'] = df_mat_tfm['material'].map(material_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_mat_tfm['scrap_rate'] = abs(df_mat_tfm['scrap_qty']) / df_mat_tfm['finish_q']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrap_rate_by_mat = (\n",
    "    df_mat_tfm.groupby('material_anon')['scrap_rate']\n",
    "              .mean()\n",
    "              .sort_values(ascending=False)\n",
    ")\n",
    "\n",
    "scrap_rate_by_mat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(9,5))\n",
    "scrap_rate_by_mat.plot(kind='bar')\n",
    "\n",
    "plt.title(\"Tasa de fallos por Material Anonimizado\")\n",
    "plt.xlabel(\"Material\")\n",
    "plt.ylabel(\"Tasa de Fallos\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (df_mat_tfm['process_line'].unique())\n",
    "line_map = {\n",
    "    'LINE_A': 'proc_x_01',\n",
    "    'LINE_B': 'proc_x_02'\n",
    "}\n",
    "\n",
    "df_mat_tfm['process_line_anon'] = df_mat_tfm['process_line'].map(line_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrap_rate_by_line = (\n",
    "    df_mat_tfm.groupby('process_line_anon')['scrap_rate']\n",
    "              .mean()\n",
    "              .sort_values(ascending=False)\n",
    ")\n",
    "\n",
    "scrap_rate_by_line\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(7,4))\n",
    "scrap_rate_by_line.plot(kind='bar', color='steelblue')\n",
    "\n",
    "plt.title(\"Tasa de Fallas por Línea de Proceso (Anonimizada)\")\n",
    "plt.xlabel(\"Línea de Proceso\")\n",
    "plt.ylabel(\"Tasa de Fallas\")\n",
    "plt.xticks(rotation=0)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "line1 = df_mat_tfm[df_mat_tfm['process_line_anon'] == 'proc_x_01']['scrap_rate']\n",
    "line2 = df_mat_tfm[df_mat_tfm['process_line_anon'] == 'proc_x_02']['scrap_rate']\n",
    "\n",
    "t_stat, p_val = stats.ttest_ind(line1, line2, equal_var=False)  # Welch’s t-test\n",
    "t_stat, p_val\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mat_tfm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tfm2= pd.read_csv(r\"./data/export_tfm2.csv\")\n",
    "\n",
    "df_tfm2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tfm2 = df_tfm2.rename(columns={'s.container_batch': 'batch'})\n",
    "df_merged = df_mat_tfm.merge(\n",
    "    df_tfm2[['batch', 'dp.data_value']],   # solo traemos bond test y batch\n",
    "    on='batch',\n",
    "    how='inner'\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.head()\n",
    "df_merged.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged['dp.data_value'] = pd.to_numeric(df_merged['dp.data_value'], errors='coerce')\n",
    "df_merged = df_merged[df_merged['dp.data_value'] <= 20]\n",
    "df_merged = df_merged.dropna(subset=['dp.data_value'])\n",
    "\n",
    "\n",
    "df_bond_mean = df_merged.groupby(['batch', 'material_anon'], as_index=False)['dp.data_value'].mean()\n",
    "\n",
    "df_bond_mean.rename(columns={'dp.data_value': 'bond_test_mean'}, inplace=True)\n",
    "\n",
    "df_bond_mean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "sns.boxplot(data=df_bond_mean, x='material_anon', y='bond_test_mean')\n",
    "plt.xlabel(\"Material Anonimizado\")\n",
    "plt.ylabel(\"Fuerza de Adhesión Promedio\")\n",
    "plt.title(\"Fuerza de Adhesión por Material (Anonimizado)\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = [g['bond_test_mean'].values \n",
    "          for name, g in df_bond_mean.groupby('material_anon')]\n",
    "\n",
    "F, p = stats.f_oneway(*groups)\n",
    "F, p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# --- Turno A/B/C según la hora ---\n",
    "def asignar_turno(fecha):\n",
    "    h = fecha.hour\n",
    "    m = fecha.minute\n",
    "    t = h*60 + m  # minutos desde medianoche\n",
    "    \n",
    "    # Turno A: 6:00–15:30  (360–930)\n",
    "    if 360 <= t < 930:\n",
    "        return \"A\"\n",
    "    # Turno B: 15:30–22:00 (930–1320)\n",
    "    elif 930 <= t < 1320:\n",
    "        return \"B\"\n",
    "    # Turno C: 22:00–6:30 (1320–390)\n",
    "    else:\n",
    "        return \"C\"\n",
    "\n",
    "# --- OrdenTurno para ordenar en Power BI ---\n",
    "def asignar_orden(turno):\n",
    "    if turno == \"A\":\n",
    "        return 1\n",
    "    elif turno == \"B\":\n",
    "        return 2\n",
    "    else:\n",
    "        return 3\n",
    "\n",
    "# --- PROCESAR EL DATAFRAME ---\n",
    "df_merged[\"scrap_date\"] = pd.to_datetime(df_merged[\"scrap_date\"])\n",
    "\n",
    "df_merged[\"Turno\"] = df_merged[\"scrap_date\"].apply(asignar_turno)\n",
    "df_merged[\"OrdenTurno\"] = df_merged[\"Turno\"].apply(asignar_orden)\n",
    "\n",
    "print(df_merged[[\"scrap_date\", \"Turno\", \"OrdenTurno\"]].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. Asegurar que scrap_date es datetime real ---\n",
    "df_merged[\"scrap_date\"] = pd.to_datetime(df_merged[\"scrap_date\"], errors=\"coerce\")\n",
    "\n",
    "# --- 3. Convertir scrap_date a minutos desde medianoche ---\n",
    "df_merged[\"time_minutes\"] = (\n",
    "    df_merged[\"scrap_date\"].dt.hour * 60 +\n",
    "    df_merged[\"scrap_date\"].dt.minute\n",
    ")\n",
    "\n",
    "# --- 4. Función para asignar Turno sin volver loca a Power BI ---\n",
    "def clasificar_turno(mins):\n",
    "    if pd.isna(mins):\n",
    "        return None\n",
    "\n",
    "    # Turno A: 6:00–15:30  (360–930)\n",
    "    if 360 <= mins < 930:\n",
    "        return \"A\"\n",
    "    # Turno B: 15:30–22:00  (930–1320)\n",
    "    elif 930 <= mins < 1320:\n",
    "        return \"B\"\n",
    "    # Turno C: 22:00–6:30 (1320–390)\n",
    "    else:\n",
    "        return \"C\"\n",
    "\n",
    "# --- 5. Crear columna Turno ---\n",
    "df_merged[\"Turno\"] = df_merged[\"time_minutes\"].apply(clasificar_turno)\n",
    "\n",
    "# --- 6. Crear columna OrdenTurno (para ordenar A-B-C sin errores) ---\n",
    "orden = {\"A\": 1, \"B\": 2, \"C\": 3}\n",
    "df_merged[\"OrdenTurno\"] = df_merged[\"Turno\"].map(orden)\n",
    "\n",
    "# --- 7. (Opcional) Eliminar columna auxiliar ---\n",
    "df_merged.drop(columns=[\"time_minutes\"], inplace=True)\n",
    "\n",
    "# --- 8. Exportar listo para Power BI ---\n",
    "df_merged.to_csv(\"df_final_para_powerbi.csv\", index=False)\n",
    "\n",
    "print(\"Archivo listo: df_final_para_powerbi.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
